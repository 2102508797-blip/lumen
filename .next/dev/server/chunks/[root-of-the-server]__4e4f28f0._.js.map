{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 46, "column": 0}, "map": {"version":3,"sources":["file:///Users/abey_aryan/Downloads/smart-calendar-app/app/api/chat/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from \"next/server\"\n\nconst SYSTEM_PROMPT = `You are a helpful AI productivity assistant integrated into a smart calendar app. Your role is to:\n- Provide personalized productivity advice\n- Help users with time management strategies\n- Give tips on effective time blocking and scheduling\n- Suggest productivity techniques and best practices\n- Help users analyze their schedule and habits\n- Motivate users to stay focused and productive\n\nBe conversational, supportive, and practical. Keep responses concise but informative. Focus on actionable advice that users can immediately implement.`\n\nexport async function POST(request: NextRequest) {\n  try {\n    const { message, conversationHistory } = await request.json()\n\n    if (!message || typeof message !== \"string\") {\n      return NextResponse.json(\n        { error: \"Invalid message\" },\n        { status: 400 }\n      )\n    }\n\n    // Determine which provider to use: Gemini (Google) or OpenAI.\n    const useGemini = String(process.env.USE_GEMINI ?? \"false\").toLowerCase() === \"true\"\n    const useOpenAI = String(process.env.USE_OPENAI ?? \"true\").toLowerCase() !== \"false\"\n\n    const geminiKey = process.env.GEMINI_API_KEY\n    const openaiKey = process.env.OPENAI_API_KEY\n\n    // If Gemini is explicitly requested and key present, call Gemini.\n    if (useGemini && geminiKey) {\n      // Build a prompt by concatenating system prompt + conversation history + user message\n      const promptParts: string[] = [SYSTEM_PROMPT]\n      if (conversationHistory && Array.isArray(conversationHistory)) {\n        for (const item of conversationHistory) {\n          const role = item.role === \"assistant\" ? \"Assistant\" : \"User\"\n          promptParts.push(`${role}: ${item.content}`)\n        }\n      }\n      promptParts.push(`User: ${message}`)\n      const promptText = promptParts.join(\"\\n\\n\")\n\n      // Allow overriding the Gemini model via env var; default to text-bison-001\n      const geminiModelRaw = String(process.env.GEMINI_MODEL ?? \"text-bison-001\").trim()\n      const modelCandidates: string[] = []\n      // Accept forms like \"text-bison-001\" or \"models/text-bison-001\"\n      if (geminiModelRaw.startsWith(\"models/\")) {\n        modelCandidates.push(geminiModelRaw)\n        modelCandidates.push(geminiModelRaw.replace(/^models\\//, \"\"))\n      } else {\n        modelCandidates.push(geminiModelRaw)\n        modelCandidates.push(`models/${geminiModelRaw}`)\n      }\n\n      // Try each candidate model path until one succeeds\n      let lastErrorBody: any = null\n      for (const candidate of modelCandidates) {\n        const geminiUrl = `https://generativelanguage.googleapis.com/v1beta2/models/${encodeURIComponent(candidate)}:generateText?key=${encodeURIComponent(geminiKey)}`\n\n        try {\n          const geminiResp = await fetch(geminiUrl, {\n            method: \"POST\",\n            headers: {\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              prompt: { text: promptText },\n              // parameters can be adjusted if desired\n              temperature: 0.7,\n              maxOutputTokens: 512,\n            }),\n          })\n\n          if (!geminiResp.ok) {\n            let errorBody: any = null\n            try {\n              errorBody = await geminiResp.json()\n            } catch (e) {\n              // ignore\n            }\n            lastErrorBody = errorBody\n            console.error(`Gemini API error for model ${candidate}:`, errorBody)\n\n            // If it's a NOT_FOUND for this candidate, try the next candidate\n            const status = errorBody?.error?.status || errorBody?.status || null\n            const code = errorBody?.error?.code || errorBody?.code || null\n            const msg = errorBody?.error?.message || errorBody?.message || \"\"\n\n            // If model-specific 404/NOT_FOUND, continue to try other candidate strings\n            if (geminiResp.status === 404 || status === \"NOT_FOUND\" || /not found/i.test(String(msg))) {\n              continue\n            }\n\n            // Map common quota / resource errors to insufficient_quota\n            if (code === \"RESOURCE_EXHAUSTED\" || /quota|exceed|exceeded/i.test(String(msg))) {\n              return NextResponse.json(\n                {\n                  error: \"insufficient_quota\",\n                  message:\n                    \"Gemini (Google) quota exceeded. Please check your Google Cloud billing and quotas or set USE_GEMINI=false to use a different provider or mock responses.\",\n                },\n                { status: 402 },\n              )\n            }\n\n            // Other errors: fall back to next candidate or to mock with warning\n            continue\n          }\n\n          // Success\n          const geminiData = await geminiResp.json().catch(() => null)\n          const geminiText = geminiData?.candidates?.[0]?.content || geminiData?.candidates?.[0]?.output?.[0]?.content || geminiData?.output?.[0]?.content || null\n\n          if (geminiText) {\n            return NextResponse.json({ response: geminiText })\n          }\n\n          // If no usable text, record warning and try next\n          lastErrorBody = { message: \"Gemini returned no text\" }\n        } catch (e) {\n          // network / unexpected error - record and continue\n          lastErrorBody = { message: String(e) }\n          console.error(\"Gemini request failed:\", e)\n        }\n      }\n\n      // If we tried candidates and none worked, provide a clear NOT_FOUND / model guidance\n      const errMsg = lastErrorBody?.error?.message || lastErrorBody?.message || \"Requested Gemini model not found or inaccessible.\"\n      return NextResponse.json(\n        {\n          error: \"gemini_model_not_found\",\n          message:\n            `Gemini model not found or inaccessible. Tried model variants: ${modelCandidates.join(\", \")}.\\nError: ${errMsg}\\nEnsure the Generative Language API is enabled and your API key has access to the requested model (and billing is enabled).`,\n        },\n        { status: 404 },\n      )\n    }\n\n    // If OpenAI is enabled and key present, use OpenAI next\n    if (!useOpenAI || !openaiKey) {\n      // Return a mock response if no provider is available or OpenAI disabled\n      return NextResponse.json({ response: generateMockResponse(message) })\n    }\n\n    // Build messages array for OpenAI\n    const messages = [\n      {\n        role: \"system\",\n        content: SYSTEM_PROMPT,\n      },\n      ...(conversationHistory || []),\n      {\n        role: \"user\",\n        content: message,\n      },\n    ]\n\n    // Call OpenAI API\n    const response = await fetch(\"https://api.openai.com/v1/chat/completions\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${openaiKey}`,\n      },\n      body: JSON.stringify({\n        model: \"gpt-3.5-turbo\",\n        messages: messages,\n        temperature: 0.7,\n        max_tokens: 500,\n      }),\n    })\n\n    if (!response.ok) {\n      // Try to parse the error body from OpenAI to provide helpful feedback\n      let errorBody: any = null\n      try {\n        errorBody = await response.json()\n      } catch (e) {\n        // ignore\n      }\n      console.error(\"OpenAI API error:\", errorBody)\n\n      const openaiErrorCode = errorBody?.error?.code || errorBody?.error?.type || null\n      // If the error indicates insufficient quota, return a clear error payload\n      if (openaiErrorCode === \"insufficient_quota\" || openaiErrorCode === \"insufficient_quota\") {\n        return NextResponse.json(\n          {\n            error: \"insufficient_quota\",\n            message:\n              \"OpenAI quota exceeded. Please check your OpenAI billing and plan at https://platform.openai.com/account/usage or set USE_OPENAI=false to use mock responses.\",\n          },\n          { status: 402 },\n        )\n      }\n\n      // For other errors, fall back to the mock response but include error info\n      return NextResponse.json({\n        response: generateMockResponse(message),\n        warning: errorBody?.error?.message || \"OpenAI API returned an error, using fallback response\",\n      })\n    }\n\n    const data = await response.json()\n    const assistantMessage = data.choices?.[0]?.message?.content || \"I couldn't generate a response.\"\n\n    return NextResponse.json({ response: assistantMessage })\n  } catch (error) {\n    console.error(\"Chat API error:\", error)\n    return NextResponse.json(\n      { error: \"Internal server error\" },\n      { status: 500 }\n    )\n  }\n}\n\n// Mock response generator for when API key is not configured\nfunction generateMockResponse(userMessage: string): string {\n  const lowerMessage = userMessage.toLowerCase()\n\n  // Time blocking tips\n  if (lowerMessage.includes(\"time block\") || lowerMessage.includes(\"schedule\")) {\n    return \"Great question about time blocking! Here are some tips:\\n\\nâœ“ Block 90 minutes for deep work sessions - this is the optimal duration for focused work\\nâœ“ Include buffer time (15-30 minutes) between meetings or classes\\nâœ“ Schedule breaks every 2 hours - even 5-10 minutes helps maintain productivity\\nâœ“ Batch similar tasks together to reduce context switching\\nâœ“ Be realistic about how long tasks actually take\\nâœ“ Protect your deep work blocks - they're precious!\"\n  }\n\n  // Productivity advice\n  if (lowerMessage.includes(\"productivity\") || lowerMessage.includes(\"focus\")) {\n    return \"Here are some proven productivity strategies:\\n\\nðŸŽ¯ Pomodoro Technique: Work for 25 minutes, then take a 5-minute break\\nðŸŽ¯ Energy Management: Schedule high-priority tasks during your peak energy hours\\nðŸŽ¯ Single-tasking: One task at a time is more effective than multitasking\\nðŸŽ¯ Remove distractions: Turn off notifications during focused work\\nðŸŽ¯ Track your progress: Seeing progress is motivating!\\n\\nWhat specific area would you like help with?\"\n  }\n\n  // Stress/overwhelm\n  if (lowerMessage.includes(\"overwhelm\") || lowerMessage.includes(\"stress\") || lowerMessage.includes(\"busy\")) {\n    return \"Feeling overwhelmed is common! Here's how to manage it:\\n\\n1. List everything on your mind - get it out of your head\\n2. Prioritize ruthlessly - focus on what truly matters\\n3. Break large projects into smaller, manageable tasks\\n4. Say no to non-essential commitments\\n5. Schedule breaks and downtime - rest is productive\\n6. Celebrate small wins - progress builds momentum\\n\\nRemember: You can't do everything. Focus on what matters most!\"\n  }\n\n  // Default response\n  return \"That's a great question! Here are some thoughts:\\n\\nâ€¢ Start by identifying your most important tasks\\nâ€¢ Block time for deep, focused work on these priorities\\nâ€¢ Use your calendar to visualize your time and spot patterns\\nâ€¢ Track which activities are actually productive for you\\nâ€¢ Adjust your schedule based on what works best\\n\\nWhat specific productivity challenge are you facing? I'm here to help!\"\n}\n"],"names":[],"mappings":";;;;AAAA;;AAEA,MAAM,gBAAgB,CAAC;;;;;;;;sJAQ+H,CAAC;AAEhJ,eAAe,KAAK,OAAoB;IAC7C,IAAI;QACF,MAAM,EAAE,OAAO,EAAE,mBAAmB,EAAE,GAAG,MAAM,QAAQ,IAAI;QAE3D,IAAI,CAAC,WAAW,OAAO,YAAY,UAAU;YAC3C,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAkB,GAC3B;gBAAE,QAAQ;YAAI;QAElB;QAEA,8DAA8D;QAC9D,MAAM,YAAY,OAAO,QAAQ,GAAG,CAAC,UAAU,IAAI,SAAS,WAAW,OAAO;QAC9E,MAAM,YAAY,OAAO,QAAQ,GAAG,CAAC,UAAU,IAAI,QAAQ,WAAW,OAAO;QAE7E,MAAM,YAAY,QAAQ,GAAG,CAAC,cAAc;QAC5C,MAAM,YAAY,QAAQ,GAAG,CAAC,cAAc;QAE5C,kEAAkE;QAClE,IAAI,aAAa,WAAW;YAC1B,sFAAsF;YACtF,MAAM,cAAwB;gBAAC;aAAc;YAC7C,IAAI,uBAAuB,MAAM,OAAO,CAAC,sBAAsB;gBAC7D,KAAK,MAAM,QAAQ,oBAAqB;oBACtC,MAAM,OAAO,KAAK,IAAI,KAAK,cAAc,cAAc;oBACvD,YAAY,IAAI,CAAC,GAAG,KAAK,EAAE,EAAE,KAAK,OAAO,EAAE;gBAC7C;YACF;YACA,YAAY,IAAI,CAAC,CAAC,MAAM,EAAE,SAAS;YACnC,MAAM,aAAa,YAAY,IAAI,CAAC;YAEpC,2EAA2E;YAC3E,MAAM,iBAAiB,OAAO,QAAQ,GAAG,CAAC,YAAY,IAAI,kBAAkB,IAAI;YAChF,MAAM,kBAA4B,EAAE;YACpC,gEAAgE;YAChE,IAAI,eAAe,UAAU,CAAC,YAAY;gBACxC,gBAAgB,IAAI,CAAC;gBACrB,gBAAgB,IAAI,CAAC,eAAe,OAAO,CAAC,aAAa;YAC3D,OAAO;gBACL,gBAAgB,IAAI,CAAC;gBACrB,gBAAgB,IAAI,CAAC,CAAC,OAAO,EAAE,gBAAgB;YACjD;YAEA,mDAAmD;YACnD,IAAI,gBAAqB;YACzB,KAAK,MAAM,aAAa,gBAAiB;gBACvC,MAAM,YAAY,CAAC,yDAAyD,EAAE,mBAAmB,WAAW,kBAAkB,EAAE,mBAAmB,YAAY;gBAE/J,IAAI;oBACF,MAAM,aAAa,MAAM,MAAM,WAAW;wBACxC,QAAQ;wBACR,SAAS;4BACP,gBAAgB;wBAClB;wBACA,MAAM,KAAK,SAAS,CAAC;4BACnB,QAAQ;gCAAE,MAAM;4BAAW;4BAC3B,wCAAwC;4BACxC,aAAa;4BACb,iBAAiB;wBACnB;oBACF;oBAEA,IAAI,CAAC,WAAW,EAAE,EAAE;wBAClB,IAAI,YAAiB;wBACrB,IAAI;4BACF,YAAY,MAAM,WAAW,IAAI;wBACnC,EAAE,OAAO,GAAG;wBACV,SAAS;wBACX;wBACA,gBAAgB;wBAChB,QAAQ,KAAK,CAAC,CAAC,2BAA2B,EAAE,UAAU,CAAC,CAAC,EAAE;wBAE1D,iEAAiE;wBACjE,MAAM,SAAS,WAAW,OAAO,UAAU,WAAW,UAAU;wBAChE,MAAM,OAAO,WAAW,OAAO,QAAQ,WAAW,QAAQ;wBAC1D,MAAM,MAAM,WAAW,OAAO,WAAW,WAAW,WAAW;wBAE/D,2EAA2E;wBAC3E,IAAI,WAAW,MAAM,KAAK,OAAO,WAAW,eAAe,aAAa,IAAI,CAAC,OAAO,OAAO;4BACzF;wBACF;wBAEA,2DAA2D;wBAC3D,IAAI,SAAS,wBAAwB,yBAAyB,IAAI,CAAC,OAAO,OAAO;4BAC/E,OAAO,gJAAY,CAAC,IAAI,CACtB;gCACE,OAAO;gCACP,SACE;4BACJ,GACA;gCAAE,QAAQ;4BAAI;wBAElB;wBAGA;oBACF;oBAEA,UAAU;oBACV,MAAM,aAAa,MAAM,WAAW,IAAI,GAAG,KAAK,CAAC,IAAM;oBACvD,MAAM,aAAa,YAAY,YAAY,CAAC,EAAE,EAAE,WAAW,YAAY,YAAY,CAAC,EAAE,EAAE,QAAQ,CAAC,EAAE,EAAE,WAAW,YAAY,QAAQ,CAAC,EAAE,EAAE,WAAW;oBAEpJ,IAAI,YAAY;wBACd,OAAO,gJAAY,CAAC,IAAI,CAAC;4BAAE,UAAU;wBAAW;oBAClD;oBAEA,iDAAiD;oBACjD,gBAAgB;wBAAE,SAAS;oBAA0B;gBACvD,EAAE,OAAO,GAAG;oBACV,mDAAmD;oBACnD,gBAAgB;wBAAE,SAAS,OAAO;oBAAG;oBACrC,QAAQ,KAAK,CAAC,0BAA0B;gBAC1C;YACF;YAEA,qFAAqF;YACrF,MAAM,SAAS,eAAe,OAAO,WAAW,eAAe,WAAW;YAC1E,OAAO,gJAAY,CAAC,IAAI,CACtB;gBACE,OAAO;gBACP,SACE,CAAC,8DAA8D,EAAE,gBAAgB,IAAI,CAAC,MAAM,UAAU,EAAE,OAAO,4HAA4H,CAAC;YAChP,GACA;gBAAE,QAAQ;YAAI;QAElB;QAEA,wDAAwD;QACxD,IAAI,CAAC,aAAa,CAAC,WAAW;YAC5B,wEAAwE;YACxE,OAAO,gJAAY,CAAC,IAAI,CAAC;gBAAE,UAAU,qBAAqB;YAAS;QACrE;QAEA,kCAAkC;QAClC,MAAM,WAAW;YACf;gBACE,MAAM;gBACN,SAAS;YACX;eACI,uBAAuB,EAAE;YAC7B;gBACE,MAAM;gBACN,SAAS;YACX;SACD;QAED,kBAAkB;QAClB,MAAM,WAAW,MAAM,MAAM,8CAA8C;YACzE,QAAQ;YACR,SAAS;gBACP,gBAAgB;gBAChB,eAAe,CAAC,OAAO,EAAE,WAAW;YACtC;YACA,MAAM,KAAK,SAAS,CAAC;gBACnB,OAAO;gBACP,UAAU;gBACV,aAAa;gBACb,YAAY;YACd;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,sEAAsE;YACtE,IAAI,YAAiB;YACrB,IAAI;gBACF,YAAY,MAAM,SAAS,IAAI;YACjC,EAAE,OAAO,GAAG;YACV,SAAS;YACX;YACA,QAAQ,KAAK,CAAC,qBAAqB;YAEnC,MAAM,kBAAkB,WAAW,OAAO,QAAQ,WAAW,OAAO,QAAQ;YAC5E,0EAA0E;YAC1E,IAAI,oBAAoB,wBAAwB,oBAAoB,sBAAsB;gBACxF,OAAO,gJAAY,CAAC,IAAI,CACtB;oBACE,OAAO;oBACP,SACE;gBACJ,GACA;oBAAE,QAAQ;gBAAI;YAElB;YAEA,0EAA0E;YAC1E,OAAO,gJAAY,CAAC,IAAI,CAAC;gBACvB,UAAU,qBAAqB;gBAC/B,SAAS,WAAW,OAAO,WAAW;YACxC;QACF;QAEA,MAAM,OAAO,MAAM,SAAS,IAAI;QAChC,MAAM,mBAAmB,KAAK,OAAO,EAAE,CAAC,EAAE,EAAE,SAAS,WAAW;QAEhE,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE,UAAU;QAAiB;IACxD,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,mBAAmB;QACjC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAAwB,GACjC;YAAE,QAAQ;QAAI;IAElB;AACF;AAEA,6DAA6D;AAC7D,SAAS,qBAAqB,WAAmB;IAC/C,MAAM,eAAe,YAAY,WAAW;IAE5C,qBAAqB;IACrB,IAAI,aAAa,QAAQ,CAAC,iBAAiB,aAAa,QAAQ,CAAC,aAAa;QAC5E,OAAO;IACT;IAEA,sBAAsB;IACtB,IAAI,aAAa,QAAQ,CAAC,mBAAmB,aAAa,QAAQ,CAAC,UAAU;QAC3E,OAAO;IACT;IAEA,mBAAmB;IACnB,IAAI,aAAa,QAAQ,CAAC,gBAAgB,aAAa,QAAQ,CAAC,aAAa,aAAa,QAAQ,CAAC,SAAS;QAC1G,OAAO;IACT;IAEA,mBAAmB;IACnB,OAAO;AACT"}}]
}